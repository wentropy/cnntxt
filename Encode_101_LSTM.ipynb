{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# In[1]:\n",
    "### For reproducibility, you have set random.seed before load keras.\n",
    "### Tensorflow are a little faster, but could not reproduce results with Keras 2.0\n",
    "# In[2]:\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from IPython.display import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = { 'ag_news': ('18wYKf0IKTu2fWUUZQ_DPonZFfh-RkUkU','ag_news_csv.zip', 'zip'),\n",
    "             'amazon_review_full': ('13quYvDyutGnWSSwhYg4CopAaj4OXxRrR','amazon_review_full_csv.zip', 'zip'),\n",
    "             'amazon_review_polarity': ('1DGJITPcVJYBD13KaA26wl6xhfYQ2Kfol','amazon_review_polarity_csv.zip', 'zip'),\n",
    "             'dbpedia': ('1zRBM3-CCms0GqVqmeuoHA4GaeFYGr2KK','dbpedia_csv.zip', 'zip'),\n",
    "             'sogou_news': ('1ghut0YEGDqHzIpevXCLx4Yi3dqHoZ9lk','sogou_news_csv.zip', 'zip'),\n",
    "             'yahoo_answers': ('1Yxp4Y_3BgK2qX2sT9e7VuSGJYTiMfpLM','yahoo_answers_csv.zip', 'zip'),\n",
    "             'yelp_review_full': ('1YDwclFFYy7vMKHPQzUJjyojp4VGht-xL','yelp_review_full_csv.zip', 'zip'),\n",
    "             'yelp_review_polarity': ('1P3N6EHdZUp1ggpRAJCt6Bd7oDDIkM9l5','yelp_review_polarity_csv.zip', 'zip')\n",
    "           }\n",
    "\n",
    "    \n",
    "#https://drive.google.com/open?id=13quYvDyutGnWSSwhYg4CopAaj4OXxRrR\n",
    "#https://drive.google.com/open?id=1DGJITPcVJYBD13KaA26wl6xhfYQ2Kfol\n",
    "#https://drive.google.com/open?id=1zRBM3-CCms0GqVqmeuoHA4GaeFYGr2KK\n",
    "#https://drive.google.com/open?id=1ghut0YEGDqHzIpevXCLx4Yi3dqHoZ9lk\n",
    "#https://drive.google.com/open?id=1Yxp4Y_3BgK2qX2sT9e7VuSGJYTiMfpLM\n",
    "#https://drive.google.com/open?id=1YDwclFFYy7vMKHPQzUJjyojp4VGht-xL\n",
    "#https://drive.google.com/open?id=1P3N6EHdZUp1ggpRAJCt6Bd7oDDIkM9l5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e01225628de452eab890efa936e8417"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wid = Dropdown(options=list(datasets), description='Select dataset to use')\n",
    "display(wid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing data and downloading if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = 'data' + os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(id, destination):\n",
    "    import requests\n",
    "    \n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbase = wid.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected dataset is ag_news\n"
     ]
    }
   ],
   "source": [
    "print('The selected dataset is', dbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_folder + dbase+'_csv'):\n",
    "    #os.mkdir(data_folder + selected_dataset)\n",
    "    id, file_name, file_type = datasets[dbase]\n",
    "    download_file_from_google_drive(id, data_folder +os.sep+file_name)\n",
    "    \n",
    "    if file_type == 'tgz':\n",
    "        import tarfile\n",
    "        tar = tarfile.open(data_folder + os.sep+file_name)\n",
    "        tar.extractall(data_folder + os.sep)\n",
    "        \n",
    "    if file_type =='zip':\n",
    "        import zipfile\n",
    "        zip_ref = zipfile.ZipFile(data_folder +os.sep+file_name, 'r')\n",
    "        zip_ref.extractall(data_folder + os.sep)\n",
    "        zip_ref.close()\n",
    "        \n",
    "    ## if file exists, delete it ##\n",
    "    if os.path.isfile(data_folder +os.sep+file_name):\n",
    "        os.remove(data_folder +os.sep+file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(7)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "if not os.path.exists('params'):\n",
    "    os.mkdir('params')\n",
    "\n",
    "\n",
    "path_test = 'data/'+dbase+'_csv/test.csv'\n",
    "path_train = 'data/'+dbase+'_csv/train.csv'\n",
    "\n",
    "epochs=10\n",
    "batch_size = 32\n",
    "batch_gen=10000\n",
    "filter_sizes = [3,3,3,3]\n",
    "\n",
    "best_acc=0\n",
    "\n",
    "cod_type='0'\n",
    "\n",
    "len_words=128\n",
    "len_cod_word=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Whether to save model parameters\n",
    "\n",
    "model_output = 'params/Huffman_'+dbase+'_output.csv'\n",
    "model_prediction_output = 'params/Huffman_'+dbase\n",
    "model_real='params/Huffman_'+dbase+'_real'\n",
    "\n",
    "model_keras_model='params/Huffman_'+dbase+'_model.json'\n",
    "model_keras_wheights='params/Huffman_'+dbase+'_weights.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def shuffle_file(path_train, path_train_shuffled):\\n    from os import listdir\\n    from os.path import isfile, join\\n\\n    #onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\\n    exist_file =isfile(path_train_shuffled)\\n\\n    doc=[]\\n    if not exist_file:\\n        with io.open(path_train, encoding='utf-8') as f:\\n            for line in f:\\n                doc.append(line)\\n\\n        p = np.random.permutation(len(doc))\\n\\n        with open(path_train_shuffled,'w', encoding='utf-8') as f:\\n            for idp in p:\\n                f.write (doc[p[idp]])\\n            f.close() \\n    \\n\\n\\nreturn (not exist_file)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def shuffle_file(path_train, path_train_shuffled):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "\n",
    "    #onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    exist_file =isfile(path_train_shuffled)\n",
    "\n",
    "    doc=[]\n",
    "    if not exist_file:\n",
    "        with io.open(path_train, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                doc.append(line)\n",
    "\n",
    "        p = np.random.permutation(len(doc))\n",
    "\n",
    "        with open(path_train_shuffled,'w', encoding='utf-8') as f:\n",
    "            for idp in p:\n",
    "                f.write (doc[p[idp]])\n",
    "            f.close() \n",
    "    \n",
    "\n",
    "\n",
    "return (not exist_file)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# verifying if already exists a shuffle training data file\\n\\ncreated_shuffled = shuffle_file(path_train, path_train_shuffled)\\n\\n# reseting random seed\\nnp.random.seed(7) \\nif created_shuffled:\\n    print('shuffled file generated ')\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train_shuffled = 'data/'+dbase+'_csv/train_shuffled.csv'\n",
    "\n",
    "'''# verifying if already exists a shuffle training data file\n",
    "\n",
    "created_shuffled = shuffle_file(path_train, path_train_shuffled)\n",
    "\n",
    "# reseting random seed\n",
    "np.random.seed(7) \n",
    "if created_shuffled:\n",
    "    print('shuffled file generated ')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet:  57\n"
     ]
    }
   ],
   "source": [
    "# generating alphabet ordered by frequency\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with io.open(path_train_shuffled, encoding='utf-8') as f:\n",
    "    c = Counter()\n",
    "    for line in f:\n",
    "        text=line.lower()\n",
    "        c += Counter(list(text))\n",
    "\n",
    "alpha=sorted(c.items(), key=lambda c: c[1],reverse=True)\n",
    "# ranked alphabet by frequency\n",
    "letter = [item[0] for item in alpha]\n",
    "print('Alphabet: ',len(letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_matrix(a, b):\n",
    "    #assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_time(start):\n",
    "    end=time.time()\n",
    "    m, s = divmod(end-start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return (\"%d:%02d:%02d\" % (h, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "       \n",
    "    #Removing consecutive spaces, lowercase words and removing whitespaces from both sides\n",
    "    \n",
    "    text=re.sub(' +', ' ', text)   \n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "know_words={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cod = {}\n",
    "decod={}\n",
    "for i, l in enumerate(letter):\n",
    "    cod[l]=\"1\"+cod_type*i+\"1\"\n",
    "    decod[\"1\"+cod_type*i+\"1\"]=l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compress(text,know_words=know_words):\n",
    "    if text in know_words:\n",
    "        huff=know_words[text]\n",
    "    else:\n",
    "        missing= \"1\"+cod_type*(len(letter)+1)+\"1\"\n",
    "        huff =''\n",
    "        for l in text:\n",
    "            huff += cod.get(l,missing)\n",
    "        know_words[text]=huff\n",
    "    return huff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(text, len_words, len_cod_word,reverse=False):\n",
    "    # Iterate over the loaded data and create a matrix of size len_cod_word x len_word\n",
    "    # Longer are cutted, shorter are padded\n",
    "    # Chars not in the vocab are encoded to a missing value.\n",
    "\n",
    "    input_data = np.zeros((len_words,len_cod_word))\n",
    "\n",
    "    # eliminating some chars\n",
    "    sent_words = clean_str(text).split() \n",
    "    \n",
    "   \n",
    "    if reverse:\n",
    "        # limiting to len_words size - treating last words as more important\n",
    "        sent_words = sent_words[-np.min([len_words, len(sent_words)]):]\n",
    "    else:\n",
    "        sent_words = sent_words[:len_words]   \n",
    "        \n",
    "    sent_array = np.zeros((len_words,len_cod_word))\n",
    "\n",
    "    counter = 0\n",
    "    for j, w in enumerate(sent_words):\n",
    "        cod_w = compress(w)\n",
    "              \n",
    "        # limmiting cod_w to len_cod_word\n",
    "        xl=len(cod_w)\n",
    "        cod_w = cod_w[:np.min([len_cod_word, len(cod_w)])]\n",
    "        \n",
    "        for i, c in enumerate(cod_w):\n",
    "            sent_array[j,i] = int(c)\n",
    "\n",
    "    input_data= sent_array\n",
    "\n",
    "    return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning Documents:  120000\n",
      "Validating Documents:  7600\n",
      "Categories:  4\n"
     ]
    }
   ],
   "source": [
    "### category number, # validating docs, # testing docs\n",
    "\n",
    "with io.open(path_train_shuffled, encoding='utf-8') as f:\n",
    "    cat_output=0\n",
    "    ndocs_t=0\n",
    "    for line in f:\n",
    "        \n",
    "        ndocs_t+=1\n",
    "        \n",
    "        data_yx=line.split(',')\n",
    "            \n",
    "        cat = data_yx[0]\n",
    "        cat=re.sub('\"', '', cat)\n",
    "        cat=int(cat)\n",
    "        \n",
    "        if cat>cat_output:\n",
    "            cat_output=cat\n",
    "\n",
    "with io.open(path_test, encoding='utf-8') as f:\n",
    "    ndocs_v=0\n",
    "    for line in f:\n",
    "        ndocs_v+=1\n",
    "\n",
    "print('Trainning Documents: ',ndocs_t)      \n",
    "print('Validating Documents: ',ndocs_v)  \n",
    "print('Categories: ',cat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "x_evaluate=[]\n",
    "y_evaluate=[]\n",
    "\n",
    "with io.open(path_train_shuffled, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        data_yx=line.split(',')\n",
    "        cat = data_yx[0]\n",
    "        cat=re.sub('\"', '', cat)\n",
    "\n",
    "        result=np.zeros((cat_output))\n",
    "        result[np.array(int(cat))-1]=1\n",
    "            \n",
    "        y_train.append(result)\n",
    "            \n",
    "        txt=\"\"\n",
    "        for col_txt in data_yx[1:]:\n",
    "            txt = txt+\" \"+col_txt \n",
    "            \n",
    "        x_train.append(txt)\n",
    "        \n",
    "with io.open(path_test, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        data_yx=line.split(',')\n",
    "        cat = data_yx[0]\n",
    "        cat=re.sub('\"', '', cat)\n",
    "\n",
    "        result=np.zeros((cat_output))\n",
    "        result[np.array(int(cat))-1]=1\n",
    "            \n",
    "        y_evaluate.append(result)\n",
    "            \n",
    "        txt=\"\"\n",
    "        for col_txt in data_yx[1:]:\n",
    "            txt = txt+\" \"+col_txt \n",
    "            \n",
    "        x_evaluate.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_sample_from_memory(x,y,cat_output,nb_sample,len_words,len_cod_word,shuffle=True):\n",
    "    \n",
    "    p = np.random.permutation(len(x))\n",
    "    p = p[:nb_sample]\n",
    "    \n",
    "    n_sample =len(p)\n",
    "    sent_data_x = np.zeros((n_sample, len_words, len_cod_word))\n",
    "    sent_data_y = np.zeros((n_sample, cat_output))\n",
    "    \n",
    "    sample =0\n",
    "    for c_sample in p:\n",
    "        sent_data_y[sample,:] = y[c_sample]\n",
    "        sent_data_x[sample,:,:] = encode_text(x[c_sample],len_words,len_cod_word)\n",
    "            \n",
    "        sample+=1\n",
    "                \n",
    "    if shuffle:\n",
    "        sent_data_x,sent_data_y = shuffle_matrix(sent_data_x,sent_data_y)\n",
    "                \n",
    "    return (sent_data_x,sent_data_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_continuous_from_memory(x,y,cat_output,batch_gen,len_words,len_cod_word,shuffle=True):\n",
    "   \n",
    "    for n_start in range(0,len(x),batch_gen):  \n",
    "            \n",
    "        n_sample =np.min((len(x)-n_start,batch_gen))\n",
    "        sent_data_x = np.zeros((n_sample, len_words, len_cod_word))\n",
    "        sent_data_y = np.zeros((n_sample, cat_output))\n",
    "\n",
    "        for sample in range(0,n_sample):\n",
    "\n",
    "            sent_data_y[sample,:] = y[n_start+sample]\n",
    "            sent_data_x[sample,:,:] = encode_text(x[n_start+sample],len_words,len_cod_word)\n",
    "\n",
    "        if shuffle:\n",
    "            sent_data_x,sent_data_y = shuffle_matrix(sent_data_x,sent_data_y)            \n",
    "        \n",
    "        yield (sent_data_x,sent_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_sample_from_file(path,cat_output,nb_sample,len_words,len_cod_word,shuffle=True):\n",
    "    \n",
    "    with io.open(path, encoding='utf-8') as f:\n",
    "        ndocs=0\n",
    "        for line in f:\n",
    "            ndocs+=1\n",
    "    \n",
    "    p = np.random.permutation(ndocs)\n",
    "    p = p[:nb_sample]\n",
    "    \n",
    "    \n",
    "    n_sample =len(p)\n",
    "    sent_data_x = np.zeros((n_sample, len_words, len_cod_word))\n",
    "    sent_data_y = np.zeros((n_sample, cat_output))\n",
    "    \n",
    "    \n",
    "    with io.open(path, encoding='utf-8') as f:\n",
    "        \n",
    "        sample = 0\n",
    "        nline =0\n",
    "        while sample<len(p):\n",
    "\n",
    "            line = f.readline()\n",
    "            \n",
    "            if nline in p:\n",
    "\n",
    "                data_yx=line.split(',')\n",
    "                cat = data_yx[0]\n",
    "                cat=re.sub('\"', '', cat)\n",
    "\n",
    "                result=np.zeros((cat_output))\n",
    "                result[np.array(int(cat))-1]=1\n",
    "            \n",
    "                sent_data_y[sample,:] = result\n",
    "            \n",
    "                txt=\"\"\n",
    "                for col_txt in data_yx[1:]:\n",
    "                    txt = txt+\" \"+col_txt \n",
    "                               \n",
    "                sent_data_x[sample,:,:] = encode_text(txt,len_words,len_cod_word)\n",
    "            \n",
    "                sample+=1\n",
    "            \n",
    "            nline+=1\n",
    "            \n",
    "            \n",
    "        if shuffle:\n",
    "            sent_data_x,sent_data_y = shuffle_matrix(sent_data_x,sent_data_y)\n",
    "                \n",
    "    return (sent_data_x,sent_data_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_continuous_from_file(path,cat_output,batch_gen,len_words,len_cod_word,shuffle=True):\n",
    "    \n",
    "    from os.path import isfile, join\n",
    "    \n",
    "    with io.open(path, encoding='utf-8') as f:\n",
    "        ndocs=0\n",
    "        for line in f:\n",
    "            ndocs+=1\n",
    "    \n",
    "    with io.open(path, encoding='utf-8') as f:\n",
    "        for n_start in range(0,ndocs,batch_gen):  \n",
    "            \n",
    "            n_sample =np.min((ndocs-n_start,batch_gen))\n",
    "            sent_data_x = np.zeros((n_sample, len_words, len_cod_word))\n",
    "            sent_data_y = np.zeros((n_sample, cat_output))\n",
    "\n",
    "            for sample in range(0,n_sample):\n",
    "\n",
    "                line = f.readline()\n",
    "                \n",
    "                data_yx=line.split(',')\n",
    "                cat = data_yx[0]\n",
    "                cat=re.sub('\"', '', cat)\n",
    "\n",
    "                result=np.zeros((cat_output))\n",
    "                result[np.array(int(cat))-1]=1\n",
    "\n",
    "                sent_data_y[sample,:] = result\n",
    "\n",
    "                txt=\"\"\n",
    "                for col_txt in data_yx[1:]:\n",
    "                    txt = txt+\" \"+col_txt \n",
    "\n",
    "                sent_data_x[sample,:,:] = encode_text(txt,len_words,len_cod_word)\n",
    "\n",
    "            if shuffle:\n",
    "                sent_data_x,sent_data_y = shuffle_matrix(sent_data_x,sent_data_y)            \n",
    "        \n",
    "            yield (sent_data_x,sent_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Preallocating 9011/11264 Mb (0.800000) on cuda0\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "#Define what the input shape looks like\n",
    "\n",
    "inputs = Input(shape=(len_words, len_cod_word,),  dtype='float32')\n",
    "l_lstm = Dropout(.1)(LSTM(300)(inputs))\n",
    "l_dense = Dense(128, activation='sigmoid')(l_lstm)\n",
    "pred = Dense(cat_output, activation='softmax')(l_dense)\n",
    " \n",
    "model = Model(inputs=inputs, outputs=pred)\n",
    "\n",
    "# Reloading Saved params\n",
    "#model.load_weights(\"crepe_model_weights.h5\", by_name=True)\n",
    "#print ('weights loaded...')\n",
    "    \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               668400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 707,444\n",
      "Trainable params: 707,444\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "logger = CSVLogger(model_output, separator=',', append=True)\n",
    "\n",
    "# random sample of at max 10000 texts, just for reference while training\n",
    "\n",
    "#xv_ref,yv_ref=gen_sample_from_file(path_test,cat_output,1000,len_words,len_cod_word,shuffle=False)\n",
    "xv_ref,yv_ref=gen_sample_from_memory(x_evaluate,y_evaluate,cat_output,1000,len_words,len_cod_word,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def evaluating(epoch,path_test,cat_output,batch_gen,len_words,len_cod_word)\n",
    "def evaluating(epoch, x_evaluate,y_evaluate,cat_output,batch_gen,len_words,len_cod_word):\n",
    "\n",
    "    #ev_batch = gen_continuous_from_file(path_test,cat_output,batch_gen,len_words,len_cod_word)\n",
    "    ev_batch = gen_continuous_from_memory(x_evaluate,y_evaluate,cat_output,batch_gen,len_words,len_cod_word)\n",
    "\n",
    "    pred=list()\n",
    "    real=list()\n",
    "    for x_ev, y_ev in ev_batch:\n",
    "        prediction = model.predict(x_ev)\n",
    "        pred=pred+list(prediction)\n",
    "        real = real +list(y_ev)\n",
    "        \n",
    "    r=np.argmax(real,axis=1)\n",
    "    p=np.argmax(pred,axis=1)\n",
    "    \n",
    "    acc = np.sum(r==p)/len(r)\n",
    "    \n",
    "    np.savetxt(model_real+'_epoch_'+str(e)+'.txt', real,fmt='%1i')\n",
    "    np.savetxt(model_prediction_output+'_epoch_'+str(e)+'_acc_'+'{:06.4f}'.format(acc)+'.txt', pred,fmt='%06.4f')\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 1.3924 - acc: 0.2550 - val_loss: 1.3953 - val_acc: 0.2390\n",
      "Epoch 0  Trained:   8.33%\n",
      "Time Elapsed:  Total: 0:01:28 Epoch:0:01:28 Batch: 0:01:17\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3924 - acc: 0.2470 - val_loss: 1.3847 - val_acc: 0.2610\n",
      "Epoch 0  Trained:  16.67%\n",
      "Time Elapsed:  Total: 0:02:04 Epoch:0:02:04 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3890 - acc: 0.2500 - val_loss: 1.3907 - val_acc: 0.2500\n",
      "Epoch 0  Trained:  25.00%\n",
      "Time Elapsed:  Total: 0:02:41 Epoch:0:02:41 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3899 - acc: 0.2543 - val_loss: 1.3971 - val_acc: 0.2540\n",
      "Epoch 0  Trained:  33.33%\n",
      "Time Elapsed:  Total: 0:03:18 Epoch:0:03:18 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3930 - acc: 0.2545 - val_loss: 1.3858 - val_acc: 0.2390\n",
      "Epoch 0  Trained:  41.67%\n",
      "Time Elapsed:  Total: 0:03:54 Epoch:0:03:54 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3891 - acc: 0.2478 - val_loss: 1.3849 - val_acc: 0.2510\n",
      "Epoch 0  Trained:  50.00%\n",
      "Time Elapsed:  Total: 0:04:32 Epoch:0:04:32 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3878 - acc: 0.2566 - val_loss: 1.3876 - val_acc: 0.2630\n",
      "Epoch 0  Trained:  58.33%\n",
      "Time Elapsed:  Total: 0:05:09 Epoch:0:05:09 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3884 - acc: 0.2482 - val_loss: 1.3897 - val_acc: 0.2570\n",
      "Epoch 0  Trained:  66.67%\n",
      "Time Elapsed:  Total: 0:05:46 Epoch:0:05:46 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3878 - acc: 0.2519 - val_loss: 1.3859 - val_acc: 0.2630\n",
      "Epoch 0  Trained:  75.00%\n",
      "Time Elapsed:  Total: 0:06:24 Epoch:0:06:24 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 1.3889 - acc: 0.2524 - val_loss: 1.4045 - val_acc: 0.2790\n",
      "Epoch 0  Trained:  83.33%\n",
      "Time Elapsed:  Total: 0:07:02 Epoch:0:07:02 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3954 - acc: 0.2561 - val_loss: 1.3875 - val_acc: 0.2580\n",
      "Epoch 0  Trained:  91.67%\n",
      "Time Elapsed:  Total: 0:07:40 Epoch:0:07:40 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 1.3914 - acc: 0.2610 - val_loss: 1.3976 - val_acc: 0.2390\n",
      "Epoch 0  Trained: 100.00%\n",
      "Time Elapsed:  Total: 0:08:19 Epoch:0:08:19 Batch: 0:00:26\n",
      "\n",
      "Epoch 0: 0.2522\n",
      "Time Elapsed:  Total: 0:08:39 Epoch:0:08:39\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 1\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 1.3876 - acc: 0.2584 - val_loss: 1.3902 - val_acc: 0.2380\n",
      "Epoch 1  Trained:   8.33%\n",
      "Time Elapsed:  Total: 0:09:18 Epoch:0:00:38 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3671 - acc: 0.3057 - val_loss: 1.2988 - val_acc: 0.3890\n",
      "Epoch 1  Trained:  16.67%\n",
      "Time Elapsed:  Total: 0:09:56 Epoch:0:01:16 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.3047 - acc: 0.3620 - val_loss: 1.2246 - val_acc: 0.4300\n",
      "Epoch 1  Trained:  25.00%\n",
      "Time Elapsed:  Total: 0:10:33 Epoch:0:01:54 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 1.2014 - acc: 0.4452 - val_loss: 1.1204 - val_acc: 0.5030\n",
      "Epoch 1  Trained:  33.33%\n",
      "Time Elapsed:  Total: 0:11:11 Epoch:0:02:32 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 1.1221 - acc: 0.5033 - val_loss: 1.0124 - val_acc: 0.5650\n",
      "Epoch 1  Trained:  41.67%\n",
      "Time Elapsed:  Total: 0:11:50 Epoch:0:03:10 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 0.9943 - acc: 0.5606 - val_loss: 0.8960 - val_acc: 0.5900\n",
      "Epoch 1  Trained:  50.00%\n",
      "Time Elapsed:  Total: 0:12:28 Epoch:0:03:49 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 27s - loss: 1.0083 - acc: 0.5325 - val_loss: 1.4428 - val_acc: 0.2360\n",
      "Epoch 1  Trained:  58.33%\n",
      "Time Elapsed:  Total: 0:13:08 Epoch:0:04:29 Batch: 0:00:27\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 27s - loss: 1.3000 - acc: 0.3446 - val_loss: 1.0089 - val_acc: 0.5780\n",
      "Epoch 1  Trained:  66.67%\n",
      "Time Elapsed:  Total: 0:13:48 Epoch:0:05:09 Batch: 0:00:27\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 0.9176 - acc: 0.6139 - val_loss: 0.7884 - val_acc: 0.6630\n",
      "Epoch 1  Trained:  75.00%\n",
      "Time Elapsed:  Total: 0:14:28 Epoch:0:05:48 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 0.8355 - acc: 0.6569 - val_loss: 0.7663 - val_acc: 0.6990\n",
      "Epoch 1  Trained:  83.33%\n",
      "Time Elapsed:  Total: 0:15:07 Epoch:0:06:28 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 0.7521 - acc: 0.6918 - val_loss: 0.7619 - val_acc: 0.6770\n",
      "Epoch 1  Trained:  91.67%\n",
      "Time Elapsed:  Total: 0:15:47 Epoch:0:07:07 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 0.7124 - acc: 0.7173 - val_loss: 0.7181 - val_acc: 0.6990\n",
      "Epoch 1  Trained: 100.00%\n",
      "Time Elapsed:  Total: 0:16:27 Epoch:0:07:48 Batch: 0:00:26\n",
      "\n",
      "Epoch 1: 0.7126\n",
      "Time Elapsed:  Total: 0:16:42 Epoch:0:08:03\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 2\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 0.6669 - acc: 0.7418 - val_loss: 0.6843 - val_acc: 0.7210\n",
      "Epoch 2  Trained:   8.33%\n",
      "Time Elapsed:  Total: 0:17:22 Epoch:0:00:39 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 0.6275 - acc: 0.7612 - val_loss: 0.5992 - val_acc: 0.7660\n",
      "Epoch 2  Trained:  16.67%\n",
      "Time Elapsed:  Total: 0:18:00 Epoch:0:01:17 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 0.5868 - acc: 0.7791 - val_loss: 0.5528 - val_acc: 0.8020\n",
      "Epoch 2  Trained:  25.00%\n",
      "Time Elapsed:  Total: 0:18:38 Epoch:0:01:55 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 0.5604 - acc: 0.7918 - val_loss: 0.5229 - val_acc: 0.8030\n",
      "Epoch 2  Trained:  33.33%\n",
      "Time Elapsed:  Total: 0:19:16 Epoch:0:02:33 Batch: 0:00:26\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 25s - loss: 0.5401 - acc: 0.8015 - val_loss: 0.5685 - val_acc: 0.7900\n",
      "Epoch 2  Trained:  41.67%\n",
      "Time Elapsed:  Total: 0:19:52 Epoch:0:03:09 Batch: 0:00:25\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      " 3072/10000 [========>.....................] - ETA: 17s - loss: 0.5174 - acc: 0.8132"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a386339615ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mbatch_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxv_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myv_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mbatch_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits.win\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits.win\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits.win\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits.win\\Anaconda3\\envs\\keras2\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits.win\\Anaconda3\\envs\\keras2\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 989\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits.win\\Anaconda3\\envs\\keras2\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    979\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (C:\\Users\\wpmar\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.16299-SP0-Intel64_Family_6_Model_158_Stepping_9_GenuineIntel-3.6.2-64\\scan_perform\\mod.cpp:6946)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits.win\\Anaconda3\\envs\\keras2\\lib\\site-packages\\theano\\gpuarray\\type.py\u001b[0m in \u001b[0;36mvalue_zeros\u001b[1;34m(self, shape)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         return pygpu.gpuarray.zeros(shape, dtype=self.typecode,\n\u001b[0m\u001b[0;32m    370\u001b[0m                                     context=self.context)\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for e in range(0,epochs):\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    trained= 0\n",
    "    print ()\n",
    "    print ('epoch:',e)\n",
    "    print ()\n",
    "    \n",
    "    #gen=gen_continuous_from_file(path_train_shuffled,cat_output,batch_gen,len_words,len_cod_word)\n",
    "    gen=gen_continuous_from_memory(x_train,y_train,cat_output,batch_gen,len_words,len_cod_word)\n",
    "\n",
    "    for x,y in gen:\n",
    "        \n",
    "        batch_start = time.time()\n",
    "        \n",
    "        model.fit(x,y,batch_size,epochs=1,verbose=1,validation_data=(xv_ref,yv_ref),callbacks=[logger])\n",
    "        \n",
    "        batch_end = time.time()\n",
    "        \n",
    "        trained+=len(x)\n",
    "        print('Epoch '+str(e)+'  Trained: {:6.2f}%'.format(100*trained/ndocs_t))\n",
    "        print('Time Elapsed:  Total: '+show_time(total_start)+' Epoch:'+show_time(epoch_start)+ ' Batch: '+show_time(batch_start) )\n",
    "        \n",
    "        \n",
    "        print ()\n",
    "    \n",
    "    #evaluating \n",
    "    acc= evaluating(e,x_evaluate,y_evaluate,cat_output,batch_gen,len_words,len_cod_word)\n",
    "\n",
    "    \n",
    "    print ('Epoch '+str(e)+': {:06.4f}'.format(acc))\n",
    "    print('Time Elapsed:  Total: '+show_time(total_start)+' Epoch:'+show_time(epoch_start))\n",
    "    \n",
    "    if acc>best_acc:\n",
    "        best_acc=acc\n",
    "\n",
    "        # serialize model to JSON\n",
    "        with open(model_keras_model, \"w\") as json_file:\n",
    "            json_file.write(model.to_json())\n",
    "            \n",
    "        # serialize best weights to HDF5\n",
    "        model.save_weights(model_keras_wheights)\n",
    "        print(\"Saved model to disk\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
